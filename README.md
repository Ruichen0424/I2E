# I2E
This is the official pytorch implementation of paper **I2E: Real-Time Image-to-Event Conversion for High-Performance Spiking Neural Networks**.

Paper Link: &emsp;
[Arxiv](https://arxiv.org/abs/2511.08065)

I2E is a pioneering framework that bridges the data scarcity gap in neuromorphic computing by enabling real-time, high-fidelity conversion of static images to event streams.
This allows spiking neural networks (SNNs) to be trained on massive standard datasets like ImageNet, achieving state-of-the-art performance.
Notably, pre-training on I2E data and transferring to the real-world CIFAR10-DVS dataset yields an unprecedented accuracy of **92.5%**, establishing a new SOTA benchmark.
This work has been accepted for **Oral Presentation** at **AAAI 2026**.



## Abstract
Spiking neural networks (SNNs) promise highly energy-efficient computing, but their adoption is hindered by a critical scarcity of event-stream data.
This work introduces I2E, an algorithmic framework that resolves this bottleneck by converting static images into high-fidelity event streams.
By simulating microsaccadic eye movements with a highly parallelized convolution, I2E achieves a conversion speed over 300x faster than prior methods, uniquely enabling on-the-fly data augmentation for SNN training.
The framework's effectiveness is demonstrated on large-scale benchmarks.
An SNN trained on the generated I2E-ImageNet dataset achieves a state-of-the-art accuracy of 60.50\%.
Critically, this work establishes a powerful sim-to-real paradigm where pre-training on synthetic I2E data and fine-tuning on the real-world CIFAR10-DVS dataset yields an unprecedented accuracy of 92.5\%.
This result validates that synthetic event data can serve as a high-fidelity proxy for real sensor data, bridging a long-standing gap in neuromorphic engineering.
By providing a scalable solution to the data problem, I2E offers a foundational toolkit for developing high-performance neuromorphic systems.
The open-source algorithm and all generated datasets are provided to accelerate research in the field.



## Visualization
Here is a visualization of the conversion process from static RGB images to dynamic event streams.
We provide four examples to illustrate the high-fidelity conversion.

<div align="center">
<img src="./assets/original_1.jpg" width="22%" alt="Original RGB Image 1">
<img src="./assets/converted_1.gif" width="22%" alt="Converted Event Stream 1">
<img src="./assets/original_2.jpg" width="22%" alt="Original RGB Image 2">
<img src="./assets/converted_2.gif" width="22%" alt="Converted Event Stream 2">
</div>
<div align="center">
<img src="./assets/original_3.jpg" width="22%" alt="Original RGB Image 3">
<img src="./assets/converted_3.gif" width="22%" alt="Converted Event Stream 3">
<img src="./assets/original_4.jpg" width="22%" alt="Original RGB Image 4">
<img src="./assets/converted_4.gif" width="22%" alt="Converted Event Stream 4">
</div>



## Datasets
We provide the converted event-based datasets generated by I2E, including **I2E-CIFAR10**, **I2E-CIFAR100**, and **I2E-ImageNet**.
These datasets significantly facilitate research on event-stream data by providing high-quality, large-scale benchmarks.
We will continuously update and release I2E versions of other common static image datasets.
If you have any requests, please feel free to open an issue.

The datasets can be downloaded from Baidu Netdisk:
- **Link**: [Baidu Netdisk](https://pan.baidu.com/s/1G1J6MG0d_NFQuoTxR7YLWQ?pwd=ItoE)
- **Password**: `ItoE`




## Requirements
- python==3.10
- pytorch==2.2.0
- torchvision==0.17.0
- spikingjelly (dev version between 0.0.0.0.14 and 0.0.0.1.0)
- timm==1.0.19

### Environment Setup
We recommend using Anaconda to create a virtual environment:

```bash
conda create -n i2e python=3.10
conda activate i2e
```

Install PyTorch 2.2.0 and Torchvision 0.17.0 based on your CUDA version:

```bash
# CUDA 11.8
conda install pytorch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 pytorch-cuda=11.8 -c pytorch -c nvidia

# CUDA 12.1
conda install pytorch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 pytorch-cuda=12.1 -c pytorch -c nvidia

# CPU Only
conda install pytorch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 cpuonly -c pytorch
```

Install other dependencies:

```bash
pip install timm==1.0.19
# Install SpikingJelly (ensure version is compatible, e.g., >0.0.0.0.14 and <0.0.0.1.0)
pip install spikingjelly
```



## Usage
### Training (Baseline-II)
To train the models using the Baseline-II setting (with full augmentation), use the following commands.
Please ensure you update the `--dataset_path` (or `-dp`) argument to point to your local dataset location.

**CIFAR-10**
```bash
python train.py -bz 128 -dp '/path/to/CIFAR10/' --dataset 'cifar10' -n 'CIFAR10' -cn 10 -e 256 --lr 0.1 --lr_min 5e-5 -wd 2e-4 --label_smooth 0.1 --model 'resnet18' --ratio 0.07 --shuffle 4 -p 30
```

**CIFAR-100**
```bash
python train.py -bz 128 -dp '/path/to/CIFAR100/' --dataset 'cifar100' -n 'CIFAR100' -cn 100 -e 256 --lr 0.1 --lr_min 5e-5 -wd 2e-4 --label_smooth 0.1 --model 'resnet18' --ratio 0.07 --shuffle 4 -p 30
```

**ImageNet**
```bash
python train.py -bz 128 -dp '/path/to/ImageNet/' --dataset 'imagenet' -n 'ImageNet' -cn 1000 -e 128 --lr 0.1 --lr_min 5e-5 -wd 1e-5 --label_smooth 0.1 --model 'resnet18' --ratio 0.12 --shuffle 4 -p 200 --multiprocessing_distributed
```



## Pre-trained Model
We provide pre-trained models for I2E-CIFAR and I2E-ImageNet.
- **Link**: [Baidu Netdisk](https://pan.baidu.com/s/1IFyfL8EwtPCEcu73xmx13Q?pwd=ItoE)
- **Password**: `ItoE`



## Main Results
The experimental settings for the methods listed below are as follows:
- **Baseline-I**: Training from scratch with minimal augmentation (random horizontal flip only).
- **Baseline-II**: Training from scratch with full augmentation (random crop, random horizontal flip, etc.), enabled by I2E's real-time conversion.
- **Transfer-I**: Fine-tuning on the target dataset after pre-training on a source dataset (Static ImageNet for I2E-ImageNet target; I2E-ImageNet for other targets).
- **Transfer-II**: Fine-tuning on the target dataset after pre-training on **I2E-CIFAR10**.

<table>

<tr>
<th>Dataset</th>
<th align="right">Structure</th>
<th align="center">Method</th>
<th align="center">Top-1 Acc</th>
<th align="center">Downloadable</th>
</tr>

<tr>
<th rowspan=3>CIFAR10-DVS</th>
<td align="center">ResNet18</td>
<td align="center">Baseline</td>
<td align="center">65.6%</td>
<td align="center">&#x2714;</td>
</tr>
<tr>
<td align="center">ResNet18</td>
<td align="center">Transfer-I</td>
<td align="center">83.1%</td>
<td align="center">&#x2714;</td>
</tr>
<tr>
<td align="center">ResNet18</td>
<td align="center">Transfer-II</td>
<td align="center">92.5%</td>
<td align="center">&#x2714;</td>
</tr>

<tr>
<th rowspan=3>I2E-CIFAR10</th>
<td align="center">ResNet18</td>
<td align="center">Baseline-I</td>
<td align="center">85.07%</td>
<td align="center">&#x2714;</td>
</tr>
<tr>
<td align="center">ResNet18</td>
<td align="center">Baseline-II</td>
<td align="center">89.23%</td>
<td align="center">&#x2714;</td>
</tr>
<tr>
<td align="center">ResNet18</td>
<td align="center">Transfer-I</td>
<td align="center">90.86%</td>
<td align="center">&#x2714;</td>
</tr>

<tr>
<th rowspan=3>I2E-CIFAR100</th>
<td align="center">ResNet18</td>
<td align="center">Baseline-I</td>
<td align="center">51.32%</td>
<td align="center">&#x2714;</td>
</tr>
<tr>
<td align="center">ResNet18</td>
<td align="center">Baseline-II</td>
<td align="center">60.68%</td>
<td align="center">&#x2714;</td>
</tr>
<tr>
<td align="center">ResNet18</td>
<td align="center">Transfer-I</td>
<td align="center">64.53%</td>
<td align="center">&#x2714;</td>
</tr>

<tr>
<th rowspan=4>I2E-ImageNet</th>
<td align="center">ResNet18</td>
<td align="center">Baseline-I</td>
<td align="center">48.30%</td>
<td align="center">&#x2714;</td>
</tr>
<tr>
<td align="center">ResNet18</td>
<td align="center">Baseline-II</td>
<td align="center">57.97%</td>
<td align="center">&#x2714;</td>
</tr>
<tr>
<td align="center">ResNet18</td>
<td align="center">Transfer-I</td>
<td align="center">59.28%</td>
<td align="center">&#x2714;</td>
</tr>
<tr>
<td align="center">MS-ResNet34</td>
<td align="center">Baseline-II</td>
<td align="center">60.50%</td>
<td align="center">&#x2714;</td>
</tr>

</table>

## Citation
If you find our code useful for your research, or use the I2E algorithm, or use the provided I2E-Datasets, please consider citing:

```
@article{ma2025i2e,
  title={I2E: Real-Time Image-to-Event Conversion for High-Performance Spiking Neural Networks},
  author={Ma, Ruichen and Meng, Liwei and Qiao, Guanchao and Ning, Ning and Liu, Yang and Hu, Shaogang},
  journal={arXiv preprint arXiv:2511.08065},
  year={2025}
}
```